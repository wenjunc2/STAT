\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amssymb,amsthm,commath,dsfont}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{xspace}
\usepackage{microtype}
\usepackage{float}
\usepackage[round]{natbib}
\usepackage{cleveref}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[breakable]{tcolorbox}
\tcbset{breakable}
\usepackage{mathtools}

\newcommand{\colbar}{\rule[-3mm]{.3mm}{1.5em}}
\newcommand{\rowbar}{\rule[.5ex]{1.5em}{.3mm}}
\DeclareMathOperator{\rank}{rank}

% following loops stolen from djhsu
\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}
% \bbA, \bbB, ...
\def\ddef#1{\expandafter\def\csname bb#1\endcsname{\ensuremath{\mathbb{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \cA, \cB, ...
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \vA, \vB, ..., \va, \vb, ...
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop

% \valpha, \vbeta, ...,  \vGamma, \vDelta, ...,
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{\csname #1\endcsname}}}}
\ddefloop {alpha}{beta}{gamma}{delta}{epsilon}{varepsilon}{zeta}{eta}{theta}{vartheta}{iota}{kappa}{lambda}{mu}{nu}{xi}{pi}{varpi}{rho}{varrho}{sigma}{varsigma}{tau}{upsilon}{phi}{varphi}{chi}{psi}{omega}{Gamma}{Delta}{Theta}{Lambda}{Xi}{Pi}{Sigma}{varSigma}{Upsilon}{Phi}{Psi}{Omega}{ell}\ddefloop

\newcommand\T{{\scriptscriptstyle\mathsf{T}}}
\def\diag{\textup{diag}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\def\SPAN{\textup{span}}
\def\tu{\textup{u}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\Z{\mathbb{Z}}
\def\be{\mathbf{e}}
\def\nf{\nabla f}
\def\veps{\varepsilon}
\def\cl{\textup{cl}}
\def\inte{\textup{int}}
\def\dom{\textup{dom}}
\def\Rad{\textup{Rad}}
\def\lsq{\ell_{\textup{sq}}}
\def\hcR{\widehat{\cR}}
\def\hcRl{\hcR_\ell}
\def\hcRlog{\hcR_{\log}}
\def\cRl{\cR_\ell}
\def\hcE{\widehat{\cE}}
\def\cEl{\cE_\ell}
\def\hcEl{\hcE_\ell}
\def\eps{\epsilon}
\def\1{\mathds{1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\def\srelu{\sigma_{\textup{r}}}
\def\vsrelu{\vec{\sigma_{\textup{r}}}}
\def\vol{\textup{vol}}
\def\vwols{\hat{\vw}_{\textrm{ols}}}
\def\llog{\ell_{\log}}
\newcommand{\pww}[1]{\hat p_{#1}}

\newcommand{\ip}[2]{\left\langle #1, #2 \right \rangle}
\newcommand{\mjt}[1]{{\color{blue}\emph\textbf{[M:}~#1~\textbf{]}}}
\newcommand{\sahand}[1]{{\color{green}\emph\textbf{[Sah:}~#1~\textbf{]}}}

\newtheorem{fact}{Fact}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{condition}{Condition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\newenvironment{Q}
{%
  \clearpage
  \item
  }
  {%
    \phantom{s} %lol doesn't work
    \bigskip
    \textbf{Solution.}
  }

  \title{CS 446 / ECE 449 --- Homework 1}
  \author{\emph{your NetID here}}
  \date{Version 1.0}

  \begin{document}
  \maketitle

  \noindent\textbf{Instructions.}
  \begin{itemize}
    \item
      Homework is due \textbf{Tuesday, February 8, at noon CST}; no late homework accepted.

    \item
      Everyone must submit individually at gradescope under \texttt{hw1} and \texttt{hw1code}.

    \item
      The ``written'' submission at \texttt{hw1} \textbf{must be typed}, and submitted in
      any format gradescope accepts (to be safe, submit a PDF).  You may use \LaTeX, Markdown,
      Google Docs, MS Word, whatever you like; but it must be typed!

    \item
      When submitting at \texttt{hw1}, gradescope will ask you to mark out boxes
      around each of your answers; please do this precisely!

    \item
      Please make sure your NetID is clear and large on the first page of the homework.

    \item
      Your solution \textbf{must} be written in your own words.
      Please see the course webpage for full academic integrity information.
      Briefly, you may have high-level discussions with at most 3 classmates,
      whose NetIDs you should place on the first page of your solutions,
      and you should cite any external reference you use; despite all this,
      your solution must be written in your own words.

    \item
      We reserve the right to reduce the auto-graded score for
      \texttt{hw1code} if we detect funny business (e.g., your solution
      lacks any algorithm and hard-codes answers you obtained from
      someone else, or simply via trial-and-error with the autograder).

    \item
      Coding problems come with suggested ``library routines''; we include these to reduce
      your time fishing around APIs, but you are free to use other APIs.

    \item
      When submitting to \texttt{hw1code}, only upload the two python files \texttt{hw1.py} and \texttt{hw1\_utils.py}. Don't upload a zip file or additional files.

  \end{itemize}
  \noindent\textbf{Version history.}
  \begin{enumerate}
    \item Initial version.
  \end{enumerate}

  \begin{enumerate}[font={\Large\bfseries},left=0pt]
    \begin{Q}
  \textbf{\Large Linear Regression/SVD.}

  Throughout this problem let $\vX$ be the $n \times d$ matrix with the feature vectors $(\vx_i)_{i = 1}^n$ as its rows. Suppose we have the singular value decomposition $\vX = \sum_{i = 1}^r s_i \vu_i \vv_i^\top$.
  \begin{enumerate}
    \item Let the training examples $\del{\vx_i}_{i = 1}^n$ be the standard basis vectors $\ve_i$ of $\R^d$ with each $\ve_i$ repeated $n_i > 0$ times having labels $\del{y_{i_j}}_{j = 1}^{n_i}$. That is, our training set is:
    \[
        \bigcup_{i = 1}^d \cbr{\del{\ve_i, y_{i_j}}}_{j = 1}^{n_i},
    \]
    where $\sum_{i = 1}^d n_i = n$. Show that for a vector $\vw$ that minimizes the empirical risk, the components $w_i$ of $\vw$ are the averages of the labels $\del{y_{i_j}}_{j = 1}^{n_i}$: $w_i = \frac{1}{n_i}\sum_{j = 1}^{n_i} y_{i_j}$.
    
    \textbf{Hint:} Write out the expression for the empirical risk with the squared loss and set the gradient equal to zero.
    
    \textbf{Remark:} This gives some intuition as to why ``regression'' originally meant ``regression towards the mean.''
    
    \item Returning to a general matrix $\vX$, show that if the label vector $\vy$ is a linear combination of the $\cbr{\vu_i}_{i = 1}^r$ then there exists a $\vw$ for which the empirical risk is zero (meaning $\vX \vw = \vy$).
    
    \textbf{Hint:} Either consider the range of $\vX$ and use the SVD, or compute the empirical risk explicitly with $\vy = \sum_{i = 1}^r a_i \vu_i$ for some constants $a_i$ and $\hat{\vw}_{\textrm{ols}} = \vX^+ \vy$.
    
    \textbf{Remark:} It's also not hard to show that if $\vy$ is not a linear combination of the $\cbr{\vu_i}_{i = 1}^r$, then the empirical risk must be nonzero.
    
    \item Show that $\vX^\top \vX$ is invertible if and only if $(\vx_i)_{i = 1}^n$ spans $\mathbb{R}^d$.
    
    \textbf{Hint:} Recall that the squares of the singular values of $\vX$ are eigenvalues of $\vX^\top \vX$.

    \textbf{Remark:} This characterizes when linear regression has a unique solution due to the normal equation (note that we always have at least one solution obtained by the pseudoinverse). We would not have had a unique solution for part (a) if we had an $n_i = 0$.
    
    \item Provide a matrix $\vX$ such that $\vX^\top \vX$ is invertible and $\vX\vX^\top$ is not. Include a formal verification of this for full points.
    
    \textbf{Hint:} Use part (c). It may be helpful to think about conditions under which a matrix is not invertible.
    
    
  \end{enumerate}
  \end{Q}


    \begin{Q}
        \textbf{\Large Logistic Loss.}

Throughout this problem, let $\llog(z) := \ln(1+\exp(-z))$ denote the logistic loss.
Recall the empirical risk $\hcR$ for logistic regression (as presented in lecture 3):
  \begin{align*}
  \hcR_{\log}(\vw) = \frac{1}{n} \sum_{k=1}^n \ln ( 1 + \exp( - y_k \vw^\top \vx_k ) ).
  \end{align*}
As in lecture, the corresponding empirical risk $\hcRlog$ can not in general be minimized
by solving for $\vw$ in $\nabla \hcRlog(\vw) = 0$, but here we will consider
a special case where it is possible.
Specifically, consider the setting of part (a) of the previous problem:
the training examples $\del{\vx_i}_{i = 1}^n$ are the standard basis vectors $\ve_i$ of $\R^d$,
each repeated respectively $n_i > 0$ times;
one difference is that now $y_{i_j} \in \{+1,-1\}$, rather than being arbitrary elements 
of $\R$.

Crucially, throughout this problem, for each $i$ suppose that $n_i\geq 2$, and there
exist at least one positive and one negative label.

\begin{enumerate}
\item
  Recall that the logistic loss suggests a conditional probability model that the
  label of a given example is $+1$:
  for any $\vw$ and any $\vx$, the corresponding conditional model $\pww{\vw}$ is
  \[
    \pww{\vw}(\vx) := \frac {1}{1 + \exp(-\vw^\T \vx)}.
  \]
  Show that in the above setting, consider a given $i$,
  \[
    \pww{\hat \vw}(\ve_i) := \frac {\envert{\cbr{ j : y_{i_j} = +1}}}{n_i},
  \]
  where $\hat \vw$ denotes the optimal solution to $\hcRlog$ (which exists in this setting).
  
  For full points, you must include and justify a derivation.

\item
  Now suppose that for some $k$, $n_k\to\infty$; specifically,
  there is only one positive pair $(\ve_k,+1)$, and $n_k-1$ negative pairs $(\ve_k,-1)$.
  correspondingly, let $\hat\vw_n$ denote the optimal solution for each sample size.

  Determine and formally prove the limiting behavior of $\|\hat \vw_n\|$
  and $\pww{\hat \vw_n}(\ve_k)$.

\item
  Now consider the case of a fixed finite $n$, but let's violate our assumption above
  regarding $n_i\geq 2$: specifically, suppose there exists $\ve_i$ with $n_i = 1$.

  We can no longer compute an optimal $\hat \vw\in\R^d$ (why not?),
  but based on the preceding reasoning, we can make a guess for $\pww{\hat \vw}$.

  What do you think it is?  And, as a consequence, do you think
  $\pww{\hat \vw}$ represents a reasonable notion of ``prediction confidence''?

  \textbf{Grading note:} Any answer to this problem which consists of at least one
  complete sentence will receive full credit.
  That said, as the use of confidences in this form is standard in machine learning,
  please consider spending a bit of time.

\end{enumerate}
\end{Q}


    \begin{Q}
      \textbf{\Large Linear Regression.}

      Recall that the empirical risk in the linear regression method is defined as $\hcR(\vw) := \frac{1}{2n}\sum_{i=1}^n (\vw^\top \vx_i - y_i)^2$, where $\vx_i \in \R^d$ is a data point and $y_i$ is an associated label.
      \begin{enumerate}
        \item Implement linear regression using gradient descent in the \texttt{linear\_gd(X, Y, lrate, num\_iter)} function of \texttt{hw1.py}. You are given as input a training set \texttt{X} as an $n \times d$ tensor, training labels \texttt{Y} as an $n \times 1$ tensor, a learning rate \texttt{lrate}, and the number of iterations of gradient descent to run \texttt{num\_iter}.  Using gradient descent, find parameters $\vw$ that minimize the empirical risk $\hcR(\vw)$. Use $\vw = 0$ as your initial parameters, and return your final $w$ as output. Prepend a column of ones to \texttt{X} in order to accommodate a bias term in $\vw$.

          \textbf{Library routines:} \texttt{torch.matmul (@), torch.tensor.shape, torch.tensor.t, torch.cat,} 

          \texttt{torch.ones, torch.zeros, torch.reshape}.

        \item Implement linear regression by using the pseudoinverse to solve for $w$ in the \texttt{linear\_normal(X,Y)} function of \texttt{hw1.py}. You are given a training set \texttt{X} as an $n \times d$ tensor and training labels \texttt{Y} as an $n \times 1$ tensor. Return your parameters $w$ as output. As before, make sure to accommodate a bias term by prepending ones to the training examples \texttt{X}.

          \textbf{Library routines:} \texttt{torch.matmul (@), torch.cat, torch.ones, torch.pinverse}.

        \item Implement the \texttt{plot\_linear()} function in \texttt{hw1.py}.  Use the provided function \texttt{hw1\_utils.load\_reg\_data()} to generate a training set \texttt{X} and training labels \texttt{Y}. Plot the curve generated by \texttt{linear\_normal()} along with the points from the data set.  Return the plot as output.  Include the plot in your written submission.

          \textbf{Library routines:} \texttt{torch.matmul (@), torch.cat, torch.ones, plt.plot, plt.scatter,}

          \texttt{plt.show, plt.gcf} where \texttt{plt} refers to the \texttt{matplotlib.pyplot} library.
      \end{enumerate}
    \end{Q}

    \begin{Q}
  \textbf{\Large Polynomial Regression.}

  In Problem 3 you constructed a linear model $\vw^\top \vx = \sum_{i=1}^d x_i w_i$.  In this problem you will use the same setup as in the previous problem, but enhance your linear model by doing a quadratic expansion of the features.  Namely, you will construct a new linear model $f_{\vw}$ with parameters
  \[
    (w_{0}, w_{01},\dots,w_{0d},w_{11}, w_{12},\dots,w_{1d},w_{22}, w_{23},\dots,w_{2d},\dots, w_{dd})^\top,
  \]
    defined by
  	\begin{align*}
  	f_{\vw}(x) = \vw^\top \phi(\vx) = w_0 + \sum_{i=1}^d w_{0i} x_i + \sum_{i\leq j}^dw_{ij} x_ix_j.
  	\end{align*}
  	
  \textbf{Warning:} If the computational complexity of your implementation is high, it may crash the autograder (try to optimize your algorithm if it does)!
  \begin{enumerate}
  \item Given a $3$-dimensional feature vector $\vx = (x_1,x_2,x_3)$, completely write out the quadratic expanded feature vector $\phi(\vx)$.
  \item Implement the \texttt{poly\_gd()} function in \texttt{hw1.py}.  The input is in the same format as it was in Problem 3.  Implement gradient descent on this training set with $\vw$ initialized to 0.  Return $\vw$ as the output with terms in this exact order: bias, linear, then quadratic.  For example, if $d = 3$ then you would return $(w_0, w_{01},w_{02},w_{03},w_{11},w_{12},w_{13},w_{22},w_{23},w_{33})$.
  
  \textbf{Library routines:} \texttt{torch.cat, torch.ones, torch.zeros, torch.stack.}
  
  \textbf{Hint:} You will want to prepend a column of ones to \texttt{X}, and append to \texttt{X} the squared features in the specified order. You can generate the squared features in the correct order (This is important! The order of the polynomial features matters for your answer to match the correct answer on GradeScope. Check the order in the problem definition above.) using a nested for loop. We don't want duplicates (e.g., $x_0 x_1$ and $x_1 x_0$ should not both be included; we should only include $x_0 x_1$ in the quadratic case).
  
  \item Implement the \texttt{poly\_normal} function in \texttt{hw1.py}.  You are given the same data set as from part (b), but this time determine $w$ by using the pseudoinverse.  Return $\vw$ in the same order as in part (b).
  
  \textbf{Library routines:} \texttt{torch.pinverse.}
  
  \textbf{Hint:} You will still need to transform the matrix \texttt{X} in the same way as in part (b).
  
  \item Implement the \texttt{plot\_poly()} function in \texttt{hw1.py}.  Use the provided function \texttt{hw1\_utils.load\_reg\_data()} to generate a training set \texttt{X} and training labels \texttt{Y}. Plot the curve generated by \texttt{poly\_normal()} along with the points from the data set.  Return the plot as output and include it in your written submission.  Compare and contrast this plot with the plot from Problem 3.  Which model appears to approximate the data better? Justify your answer.
  
  \textbf{Library routines:} \texttt{plt.plot, plt.scatter, plt.show, plt.gcf.}
  
  \item The Minsky-Papert XOR problem is a classification problem with data set: \begin{align*}
  X = \{(-1,+1), (+1,-1), (-1,-1),(+1,+1)\}
\end{align*}
where the label for a given point $(x_1,x_2)$ is given by its product $x_1x_2$.  For example, the point $(-1,+1)$ would be given label $y = (-1)(1) = -1$.  Implement the \texttt{poly\_xor()} function in \texttt{hw1.py}.  In this function you will load the XOR data set by calling the \texttt{hw1\_utils.load\_xor\_data()} function, and then apply the \texttt{linear\_normal()} and \texttt{poly\_normal()} functions to generate predictions for the XOR points. Include a plot of contour lines that show how each model classifies points in your written submission. Return the predictions for both the linear model and the polynomial model and use \texttt{contour\_plot()} in \texttt{hw1\_utils.py} to help with the plot. Do both models correctly classify all points? (Note that red corresponds to larger values and blue to smaller values when using \texttt{contour\_plot} with the ``coolwarm" colormap).

\textbf{Hint:} A ``Contour plot" is a way to represent a 3-dimensional surface in a 2-D figure. In this example, the data points are pined to the figure with their features $(x_1, x_2)$ as the coordinates in 2-D space (e.g., x and y axis); the third dimension (e.g., the predictions of the data points) is labeled on the points in the figure. The lines or curves that link the grid points with the same predictions together are called the ``contours". See \texttt{contour\_plot()} in \texttt{hw1\_utils.py} for details.
  \end{enumerate}
  \end{Q}

    \begin{Q}
  \textbf{\Large Logistic Regression.}

  Recall the empirical risk $\hcR$ for logistic regression (as presented in lecture 3):
  \begin{align*}
  \hcR_{\log}(\vw) = \frac{1}{n} \sum_{i=1}^n \ln ( 1 + \exp( - y_i \vw^\top \vx_i ) ).
  \end{align*}
  Here you will minimize this risk using gradient descent.
  \begin{enumerate}
  \item In your written submission, derive the gradient descent update rule for this empirical risk by taking the gradient.  Write your answer in terms of the learning rate $\eta$, previous parameters $\vw$, new parameters $\vw'$, number of examples $n$, and training examples $\vx_i$.  Show all of your steps.
  \item Implement the \texttt{logistic()} function in \texttt{hw1.py}.  You are given as input a training set \texttt{X}, training labels \texttt{Y}, a learning rate \texttt{lrate}, and number of gradient updates \texttt{num\_iter}.  Implement gradient descent to find parameters $\vw$ that minimize the empirical risk $\hcR_{\log}(\vw)$. Perform gradient descent for \texttt{num\_iter} updates with a learning rate of \texttt{lrate}, initializing $\vw = 0$ and returning $\vw$ as output. Don't forget to prepend \texttt{X} with a column of ones.
  
  \textbf{Library routines:} \texttt{torch.matmul (@), torch.tensor.t, torch.exp.}
  
  \item Implement the \texttt{logistic\_vs\_ols()} function in \texttt{hw1.py}. Use \texttt{hw1\_utils.load\_logistic\_data()} to generate a training set \texttt{X} and training labels \texttt{Y}.  Run \texttt{logistic(X,Y)} from part (b) taking \texttt{X} and \texttt{Y} as input to obtain parameters $\vw$ (use the defaults for \texttt{num\_iter} and \texttt{lrate}).  Also run \texttt{linear\_gd(X,Y)} from Problem 3 to obtain parameters $\vw$.  Plot the decision boundaries for your logistic regression and least squares models along with the data \texttt{X}. Which model appears to classify the data better? Explain why you believe your choice is the better classifier for this problem.
  
  \textbf{Library routines:} \texttt{torch.linspace, plt.scatter, plt.plot, plt.show, plt.gcf.}
  
  \textbf{Hints:}
  \begin{itemize}
      \item The positive and negative points are guaranteed to be linearly separable (though an algorithm may or may not find the optimal line to separate them).
      \item The ``decision boundary" in the problem description refers to the set of points $\vx$ such that $\vw^\top \vx = 0$ for the chosen predictor. In this case, it suffices to plot the corresponding line.
      \item In order to make the two models significantly different, we recommend that you train the logistic regression with a large \texttt{num\_iter} (e.g., 1,000,000 or even larger).
  \end{itemize}
  \end{enumerate}
  \end{Q}
\end{enumerate}
\end{document}

